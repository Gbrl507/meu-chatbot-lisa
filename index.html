const express = require('express');
const app = express();

app.use(express.json());

// Função que chama o Ollama (modelo local)
async function consultarIA(mensagem) {
  const resp = await fetch("http://localhost:11434/api/generate", {
    method: "POST",
    headers: { "Content-Type": "application/json" },
    body: JSON.stringify({
      model: "llama3",
      prompt: mensagem
    })
  });

  const dados = await resp.json();

  // Extrai o texto da resposta de forma tolerante (depende do formato do Ollama)
  if (!dados) return "Sem resposta";
  if (typeof dados === "string") return dados;
  if (dados.output) return dados.output;
  if (dados.response) return dados.response;
  if (Array.isArray(dados) && dados[0] && dados[0].generated_text) return dados[0].generated_text;
  // fallback
  return JSON.stringify(dados);
}

// Rota que o index.html usa: POST /chat { message: "..." }
app.post('/chat', async (req, res) => {
  const userMessage = req.body.message;
  if (!userMessage) return res.status(400).json({ error: "Campo 'message' é obrigatório." });

  try {
    const resposta = await consultarIA(userMessage);
    // Mantemos o formato que o index.html espera: { response: "texto" }
    return res.json({ response: resposta });
  } catch (err) {
    console.error("Erro ao chamar a IA:", err);
    return res.status(500).json({ error: "Erro interno no servidor" });
  }
});

app.get('/', (req, res) => res.send('Chatbot Lisa - server OK'));

app.listen(3000, () => {
  console.log('Servidor rodando em http://localhost:3000');
});
